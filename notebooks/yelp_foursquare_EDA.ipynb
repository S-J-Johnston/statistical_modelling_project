{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = bike_stations['latitude']\n",
    "long = bike_stations['longitude']\n",
    "station_count = len(bike_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foursquare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a request to Foursquare with a small radius (1000m) for all the bike stations in your city of choice.\n",
    "\n",
    "For the API request below a reduced radius of 500m was applied. Dublin was chosen as my city and the city centre where the bicycle stations are located is pretty small. A radius of 1000m essentially covers the entire city. As such it wouldn't provide much insight about immediate area surrounding the bicycle stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FSQ_POIs(lat, long):\n",
    " #API request fields   \n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    headers['Authorization'] = \"fsq3iSSoaBgIJQOOuI1ttXRR7jgaVDhI6ZVEOuZbji/avyE=\"\n",
    "    fields = 'name,categories,distance,rating,stats,geocodes,location'\n",
    "    gps = str(lat) + ',' + str(long)\n",
    "    url = 'https://api.foursquare.com/v3/places/search?ll='+ gps + '&radius=500&limit=50&fields='+ fields\n",
    "\n",
    "  #API request \n",
    "    FSQ_api_request = requests.get(url, headers=headers)\n",
    "  \n",
    "  #Dataframe generation\n",
    "    FSQ_near_by_json = FSQ_api_request.json()\n",
    "    FSQ_near_by = pd.json_normalize(FSQ_near_by_json, record_path='results')\n",
    "\n",
    "  #Addition of unique reference for each bike station  \n",
    "    gps_series = pd.Series(gps)\n",
    "    gps_ref_df = gps_series.repeat(len(FSQ_near_by)).to_frame()\n",
    "    gps_ref_df = gps_ref_df.rename(columns={0:'gps_ref'})\n",
    "    gps_ref_df = gps_ref_df.reset_index()\n",
    "    FSQ_near_by = pd.concat([FSQ_near_by, gps_ref_df], axis=1)\n",
    "    \n",
    "  #Cleaning\n",
    "    FSQ_near_by =  FSQ_near_by[['gps_ref','name', 'distance', 'rating', 'stats.total_ratings','categories', 'geocodes.main.latitude', 'geocodes.main.longitude', 'location.formatted_address']]\n",
    "    FSQ_near_by = FSQ_near_by.rename(columns={'stats.total_ratings':'total_ratings', 'geocodes.main.latitude':'lattitude', 'geocodes.main.longitude':'longitude', 'location.formatted_address':'address'})\n",
    "    return FSQ_near_by\n",
    "\n",
    "#Repeat for all bike stations\n",
    "FSQ_dfs_list = []\n",
    "for i in range(station_count):\n",
    "  poi_df = FSQ_POIs(lat[i], long[i])\n",
    "  FSQ_dfs_list.append(poi_df)\n",
    "\n",
    "FSQ_df = x = pd.DataFrame()\n",
    "x = x = pd.DataFrame()\n",
    "\n",
    "# concatenate full df object of all stn site data:\n",
    "for poi_df in FSQ_dfs_list:\n",
    "    x = pd.concat([FSQ_df, poi_df])\n",
    "    FSQ_df = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse through the response to get the POI (such as restaurants, bars, etc) details you want (ratings, name, location, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example filter for 'pub'\n",
    "FSQ_df['categories'] = FSQ_df['categories'].apply(str)\n",
    "FSQ_df.loc[FSQ_df['categories'].str.contains(\"pub\", case=False)]\n",
    "\n",
    "#example filter for 'cafe'\n",
    "FSQ_df['categories'] = FSQ_df['categories'].apply(str)\n",
    "FSQ_df.loc[FSQ_df['categories'].str.contains(\"cafe\", case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your parsed results into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my analysis I created summarised dataframes for each API based on a select number of categories and the attributes of those categories. I then created a unique id for each bike station (gps_ref) and grouped the summaries based on those ids. An example is shown below for 'cafes' near bike stations. There definitely is a more efficient way to achieve this, as you can see below there is a good deal of manipulation of column titles and indexes required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse out field of interest\n",
    "FSQ_df['categories'] = FSQ_df['categories'].apply(str)\n",
    "FSQ_df_cafes = FSQ_df.loc[FSQ_df['categories'].str.contains(\"cafe\", case=False)]\n",
    "# Create a series for attribut of interest grouped by gps_ref of bike station\n",
    "FSQ_cafes = FSQ_df_cafes['gps_ref'].value_counts()\n",
    "FSQ_cafe_rating = FSQ_df_cafes.groupby(['gps_ref'])['rating'].mean()\n",
    "\n",
    "#Merge results into a dataframe\n",
    "FSQ_cafe_rating = pd.DataFrame(FSQ_cafe_rating)\n",
    "FSQ_cafe_rating = FSQ_cafe_rating.reset_index()\n",
    "FSQ_cafe_rating = FSQ_cafe_rating.rename(columns={'rating':'FSQ_avg_cafe_rating'})\n",
    "FSQ_cafes = pd.DataFrame(FSQ_cafes)\n",
    "FSQ_cafes = FSQ_cafes.reset_index()\n",
    "FSQ_cafes = FSQ_cafes.rename(columns={'index':'gps_ref', 'gps_ref':'FSQ_cafe_count'})\n",
    "\n",
    "#Repeat above for Pubs\n",
    "FSQ_pub_count = FSQ_df_pubs['gps_ref'].value_counts()\n",
    "FSQ_pub_count = pd.DataFrame(FSQ_pub_count)\n",
    "FSQ_pub_count = FSQ_pub_count.reset_index()\n",
    "FSQ_pub_count = FSQ_pub_count.rename(columns={'index':'gps_ref', 'gps_ref':'FSQ_pub_count'})\n",
    "FSQ_pub_rating = FSQ_df_pubs.groupby(['gps_ref'])['rating'].mean()\n",
    "FSQ_pub_rating = FSQ_pub_rating.reset_index()\n",
    "FSQ_pub_rating = FSQ_pub_rating.rename(columns={'rating':'FSQ_avg_pub_rating'})\n",
    "\n",
    "#Merge summaries into dataframe\n",
    "FSQ_cafe_summary_df = FSQ_cafes.merge(FSQ_cafe_rating, how='left', on='gps_ref')\n",
    "FSQ_pub_summary_df = FSQ_pub_count.merge(FSQ_pub_rating, how='left', on='gps_ref')\n",
    "FSQ_summary_df = FSQ_pub_summary_df.merge(FSQ_cafe_summary_df, how='left', on='gps_ref')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a request to Yelp with a small radius (1000m) for all the bike stations in your city of choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Yelp_POIs (lat, long):\n",
    "  # API request fields\n",
    "    headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer Bz_Eh2Iu4T3_tcMGLS8cbd9Ctafvk8kAyh4HZLNfvrDEQlyNZdVhX-nmaCoff4Wzz6Gf2tSBx3zmH0XkfG2gFbZ_ejxdIiLW6hhAU0r36DXLNJkVIxu0nG3X27iLY3Yx\"}\n",
    "    gps_yelp = 'latitude='+str(lat)+'&longitude='+str(long)\n",
    "    url = \"https://api.yelp.com/v3/businesses/search?\"+gps_yelp+\"&radius=500&sort_by=best_match&limit=50\"\n",
    "\n",
    "  # API request\n",
    "    yelp_api_request = requests.get(url, headers=headers)\n",
    "    \n",
    "  # Dataframe generation\n",
    "    yelp_near_by_json = yelp_api_request.json()\n",
    "    yelp_near_by_df = pd.json_normalize(yelp_near_by_json, record_path = 'businesses')\n",
    "    \n",
    "  #Addition of unique reference for each bike station \n",
    "    gps = str(lat) + ',' + str(long)\n",
    "    gps_series = pd.Series(gps)\n",
    "    gps_ref_df = gps_series.repeat(len(yelp_near_by_df)).to_frame()\n",
    "    gps_ref_df = gps_ref_df.rename(columns={0:'gps_ref'})\n",
    "    gps_ref_df = gps_ref_df.reset_index()\n",
    "    yelp_near_by_df = pd.concat([yelp_near_by_df, gps_ref_df], axis=1)\n",
    "\n",
    "  #Cleaning\n",
    "    yelp_near_by_df = yelp_near_by_df.drop(columns=['index', 'id', 'alias', 'image_url', 'is_closed', 'url', 'transactions', 'location.address1', 'location.address2', 'location.address3', 'location.city', 'location.zip_code', 'location.country', 'location.state', 'price', 'phone', 'display_phone'])\n",
    "    yelp_near_by_df = yelp_near_by_df.rename(columns={'review_count':'rating_count','coordinates.latitude':'lattitude', 'coordinates.longitude':'longitude', 'location.display_address':'address'})\n",
    "    yelp_near_by_df = yelp_near_by_df[['gps_ref','name', 'distance', 'rating', 'rating_count','categories', 'lattitude', 'longitude', 'address']]\n",
    "    return yelp_near_by_df\n",
    "\n",
    "#Repeat for all bike stations\n",
    "Yelp_dfs_list = []\n",
    "for i in range(station_count):\n",
    "  poi_df = Yelp_POIs(lat[i], long[i])\n",
    "  Yelp_dfs_list.append(poi_df)\n",
    "\n",
    "\n",
    "Yelp_df = x = pd.DataFrame()\n",
    "x = x = pd.DataFrame()\n",
    "\n",
    "# concatenate full df object of all stn site data:\n",
    "for poi_df in Yelp_dfs_list:\n",
    "    x = pd.concat([Yelp_df, poi_df])\n",
    "    Yelp_df = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse through the response to get the POI (such as restaurants, bars, etc) details you want (ratings, name, location, etc)\n",
    "\n",
    "As with FSQ, I parsed out Pubs and Cafes as categories and created attributes for average rating and the number of each category near each bike station and summarised it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse out Pubs\n",
    "Yelp_df['categories'] = Yelp_df['categories'].apply(str)\n",
    "yelp_df_pubs = Yelp_df.loc[Yelp_df['categories'].str.contains(\"pub\", case=False)]\n",
    "\n",
    "#Define attributes\n",
    "yelp_pubs = yelp_df_pubs['gps_ref'].value_counts()\n",
    "yelp_pub_rating = yelp_df_pubs.groupby(['gps_ref'])['rating'].mean()\n",
    "\n",
    "#Parse out cafes\n",
    "yelp_df_cafes = Yelp_df.loc[Yelp_df['categories'].str.contains(\"cafe\", case=False)]\n",
    "\n",
    "#Define attributes\n",
    "yelp_cafes = yelp_df_cafes['gps_ref'].value_counts()\n",
    "yelp_cafe_rating = yelp_df_cafes.groupby(['gps_ref'])['rating'].mean()\n",
    "\n",
    "#Merge into a summary dataframe\n",
    "yelp_summary_df = pd.concat([yelp_cafes, yelp_cafe_rating], axis=1)\n",
    "yelp_summary_df = yelp_summary_df.rename(columns={'index':'gps_ref', 'gps_ref':'yelp_cafe_count', 'rating':'yelp_avg_cafe_rating'})\n",
    "\n",
    "yelp_summary_df = pd.concat([yelp_summary_df, yelp_pubs, yelp_pub_rating], axis=1)\n",
    "yelp_summary_df = yelp_summary_df.reset_index()\n",
    "yelp_summary_df = yelp_summary_df.rename(columns={'index':'gps_ref', 'gps_ref':'yelp_pub_count', 'rating':'yelp_avg_pub_rating'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your parsed results into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which API provided you with more complete data? Provide an explanation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the application of these APIs would be location dependent. When you compare the number of ratings available for FSQ versus Yelp we can see there are far more ratings captured using FSQ. It was also observed for Yelp that when searching for 'restaurants' only 6 unique values were returned. There are more than 6 restaurants in Dublin city centre. This suggests to me that Yelps data is perhaps less 'complete' than FSQ when looking for places in Dublin. Perhaps Yelp is not widely used in Ireland?\n",
    "What confounds comparing the APIs as well is that they are both limited to 50 responses per request. What this means is that with a broad search we only get a sample of the places in a certain category returned. For example you may get back 5 cafes on one search and 10 the next time you execute it. All this is to say is that the responses for a given category will vary and this will cause inaccuracies in the conclusions made from them.\n",
    "One potential way around this is to tune the request to look for a single category in the hope of getting a complete sample but unfortunately there was not enough time to completed this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top 10 restaurants according to their rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 restaurants FSQ\n",
    "FSQ_df_restaurants = (FSQ_df.loc[FSQ_df['categories'].str.contains(\"restaurant\", case=False)]).drop_duplicates(subset=['name']).sort_values(by='rating', ascending = False)\n",
    "FSQ_df_restaurants.head(10)\n",
    "\n",
    "#Top 10 restaurants Yelp\n",
    "Yelp_df_restaurants = (Yelp_df.loc[Yelp_df['categories'].str.contains(\"restaurant\", case=False)]).drop_duplicates(subset=['name']).sort_values(by='rating', ascending = False)\n",
    "Yelp_df_restaurants.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
